{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21835631",
   "metadata": {},
   "source": [
    "# Classification model evaluation\n",
    "\n",
    "The purpose of this lesson is to learn how to evaluate the performance of classification models.\n",
    "\n",
    "How do we know if our model is performing well? It's important to establish a baseline against which we can compare the performance of our model. For classification problems, the baseline is often the most prevalent (mode) label present in our dataset. Our baseline \"model\" simply predicts the most abundant label for every observation. I will demonstrate how to create a baseline in a dataframe.\n",
    "\n",
    "How do we compare the performance of our models to our baseline? We need a way to quantify model and baseline performance. We'll cover three common evaluation metrics: accuracy, precision, and recall. These are a few of the most common evaluation metrics used for classification models. Once we've computed these metrics for our models, we can compare them to identify the top performer.\n",
    "\n",
    "How do we compute these evaluation metrics? We will generate the aptly named confusion matrix and use it to calculate the accuracy, precision, and recall. I will explain how to calculate each metric and we will do it by hand!\n",
    "\n",
    "Will we always have to calculate these by hand? No, that's the beauty of computers. At the end of the lesson, we'll learn how to automatically calculate these metrics using the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54941619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63d411",
   "metadata": {},
   "source": [
    "Let's create sample model predictions using everyone here today. I'll input whether or not each person is wearing glasses, and then generate some model \"predictions\" over this same dataset (it's just me making bad guesses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e11be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glasses</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  glasses preds\n",
       "0     yes   yes\n",
       "1      no    no\n",
       "2     yes   yes\n",
       "3     yes    no\n",
       "4     yes   yes\n",
       "5      no   yes\n",
       "6     yes    no\n",
       "7     yes   yes\n",
       "8      no   yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example df\n",
    "df = pd.DataFrame({'glasses': ['yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'no'],\n",
    "                   'preds': ['yes', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'yes', 'yes']})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1bd960",
   "metadata": {},
   "source": [
    "Before we can evaluate the quality of predictions made by our robust classification model, we need to establish a baseline. Our baseline will be the most abundant (mode) label in our dataset (the glasses column). In our example, we could count the number of instances of yes and no. When dealing with a much larger dataset, it's important to do these operations programmatically! We can use the pandas series method .mode() to get the baseline prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7d26f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    yes\n",
       "Name: glasses, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "df.glasses.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4954c1",
   "metadata": {},
   "source": [
    "We should add the baseline prediction to the dataframe as a new column. This will make it more convenient for us to evaluate model performance as we work through the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d985977d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glasses</th>\n",
       "      <th>preds</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  glasses preds baseline\n",
       "0     yes   yes      yes\n",
       "1      no    no      yes\n",
       "2     yes   yes      yes\n",
       "3     yes    no      yes\n",
       "4     yes   yes      yes\n",
       "5      no   yes      yes\n",
       "6     yes    no      yes\n",
       "7     yes   yes      yes\n",
       "8      no   yes      yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add baseline to df\n",
    "df['baseline'] = df.glasses.mode()[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffcb899",
   "metadata": {},
   "source": [
    "Now that we have the actual labels, our model's predictions, and the baseline in the same dataframe, let's analyze the performance of the baseline.\n",
    "\n",
    "Why evaluate the baseline first? It's nice to know the baseline at the beginning of model evaluation, because then we know what benchmarks we are attempting to beat!\n",
    "\n",
    "To evaluate our baseline, we'll create a confusion matrix. This is done with pandas using the pd.crosstab() function. We will create a crosstab between the actual values (glasses column) and our baseline's predicted values (baseline column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f6a828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>baseline</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glasses</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "baseline  yes\n",
       "glasses      \n",
       "no          3\n",
       "yes         6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline crosstab\n",
    "pd.crosstab(df.glasses, df.baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25843f3c",
   "metadata": {},
   "source": [
    "### Baseline evaluation\n",
    "\n",
    "Our crosstab is small because our baseline model only predicts one label (yes). The rows represent the actual labels from our dataset. We can see that 6 people in the room have glasses and 3 people don't. The columns represent the predictions made by our baseline model. It predicted \"yes\" 9 times (adding down the column), and it was correct 6 of those times. Let's calculate the accuracy, precision, and recall for our baseline model.\n",
    "\n",
    "Accuracy: the number of correct guesses made over all guesses.  \n",
    "In our case, the baseline accuracy is 6/9 or 67%.\n",
    "\n",
    "Precision: the number of true positive guesses over the total number of positive guesses.  \n",
    "In our case, we are treating glasses as the positive case. A true positive (TP) is when our model guesses glasses and is correct. A false positive (FP) is when our model guesses glasses and is wrong.  \n",
    "The equation for precision is often written as TP / TP + FP.  \n",
    "Using the equation, our baseline precision is 6/9 or 67%.\n",
    "\n",
    "Recall: the number of true positives identified over all true positives present in the dataset.  \n",
    "Again, we are treating having glasses as the positive case. A false negative (FN) is when our model guesses somebody is not wearing glasses but they are.  \n",
    "The equation for recall is often written as TP / TP + FN.\n",
    "Using the equation, our baseline recall is 6/6 or 100%.\n",
    "\n",
    "Now that we've evaluated our baseline, let's see how our predictive model measures up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f370ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>preds</th>\n",
       "      <th>no</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glasses</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "preds    no  yes\n",
       "glasses         \n",
       "no        1    2\n",
       "yes       2    4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model crosstab\n",
    "pd.crosstab(df.glasses, df.preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3ba2f",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We have an additional column in our confusion matrix! Unlike the example with the baseline, our model guessed both possible labels (yes and no). Let's work through the confusion matrix and calculate the same evaluation metrics for our predictive model.\n",
    "\n",
    "Accuracy: we need to add the true negatives (where \"no\" intersects) and true positives (where \"yes\" intersects) and divide by all guesses.  \n",
    "We have 1 true negative and 4 true positives, which means we divide 5 by 9 to get an accuracy of 5/9 or 56%.\n",
    "\n",
    "Precision: we are still treating having glasses as the positive case. Remember, our equation for precision is TP / TP + FP.  \n",
    "We have 4 true positives. How do we identify false positives? These are instances where our model predicted \"yes\", but the correct label was \"no\". Looking at the table, we follow the first row (no glasses) to the second column (predicted yes glasses) to see there are two false positives.  \n",
    "Our precision is 4/6 or 67%.\n",
    "\n",
    "Recall: our equation for recall is TP / TP + FN.  \n",
    "We know we have 4 true positives from calculating the precision. How many false negatives do we have? We need to look at the second row (yes glasses) and the first column (we predicted they aren't wearing glasses). Our model made 2 false negative predictions.  \n",
    "Our recall is also 4/6 or 67%.\n",
    "\n",
    "How does our predictive model compare to the baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9964938",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "The baseline had better accuracy than the model, and the same precision and recall. We would conclude that our model is NOT outperforming the baseline and is unfit for deployment.\n",
    "\n",
    "Why do we have more evaluation metrics? Isn't accuracy enough? Precision and recall optimize our models in different ways, and they have important real-life applications.\n",
    "\n",
    "In many cases, accuracy is good enough. Out of all the guesses made by our model, how many were correct?\n",
    "\n",
    "Why precision? Optimizing for precision will minimize false positives. Precision is used when we determine that a false positive result would carry the most consequences!\n",
    "\n",
    "On the other hand, optimizing for recall will minimize false negatives. We want high recall when the false negative result is the most damaging!\n",
    "\n",
    "How can we calculate these metrics more efficiently with the sklearn library?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ac6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.33      0.33      0.33         3\n",
      "         yes       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.56         9\n",
      "   macro avg       0.50      0.50      0.50         9\n",
      "weighted avg       0.56      0.56      0.56         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model eval with sklearn\n",
    "print(classification_report(df.glasses, df.preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15765cb",
   "metadata": {},
   "source": [
    "One function calculates everything! Note how precision and recall are calculated treating each possible label as the positive class. Since we treated the \"yes\" label as the positive class in our calculations, we can see how that aligns with the second row of the classification report. There are some other metrics present in the report, including the f1-score. The f1-score is another prominent evaluation metric for classification problems. If you enjoyed this lesson on classification model evaluation, I encourage you to research additional evaluation metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e99ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
